#!/usr/bin/env python3
"""
å¤šæºæ–°é—»èšåˆæœç´¢æ¨¡å— v2.0
é›†æˆçŸ¥è¯†æ˜Ÿçƒä¼˜åŒ–ç‰ˆæœç´¢åŠŸèƒ½ï¼ˆæ”¯æŒkeywordå‚æ•°ï¼‰
"""

import sys
import subprocess
import re
import requests
import urllib.parse
import time
from typing import List, Dict, Optional
from datetime import datetime

sys.path.insert(0, '/root/.openclaw/workspace/tools')
sys.path.insert(0, '/root/.openclaw/workspace')


class ZsxqSearcher:
    """çŸ¥è¯†æ˜Ÿçƒæœç´¢å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.cookie = 'sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%22421882554581888%22%2C%22first_id%22%3A%2219957298c826cb-08f4b144c21fe3-1f525631-1484784-19957298c83903%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%AB%99%E6%B5%81%E9%87%8F%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC%22%2C%22%24latest_referrer%22%3A%22https%3A%2F%2Fopen.weixin.qq.com%2F%22%7D%2C%22identities%22%3A%22eyIkaWRlbnRpdHlfY29va2llX2lkIjoiMTk5NTcyOThjODI2Y2ItMDhmNGIxNDRjMjFmZTMtMWY1MjU2MzEtMTQ4NDc4NC0xOTk1NzI5OGM4MzkwMyIsIiRpZGVudGl0eV9sb2dpbl9pZCI6IjQyMTg4MjU1NDU4MTg4OCJ9%22%2C%22history_login_id%22%3A%7B%22name%22%3A%22%24identity_login_id%22%2C%22value%22%3A%22421882554581888%22%7D%2C%22%24device_id%22%3A%2219957298c826cb-08f4b144c21fe3-1f525631-1484784-19957298c83903%22%7D; abtest_env=product; zsxq_access_token=26FC1241-0A1A-42BF-87B9-BE97A4A42AB1_2ECB6A0A4CD9622F'
        self.headers = {
            'Cookie': self.cookie,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json'
        }
        self.group_id = '28855458518111'
        self.last_query_time = None
    
    def _check_interval(self):
        """æ£€æŸ¥è¯·æ±‚é—´éš”ï¼ˆé¢‘ç‡æ§åˆ¶ï¼‰"""
        if self.last_query_time is not None:
            elapsed = (datetime.now() - self.last_query_time).total_seconds()
            if elapsed < 3:  # æœ€å°‘3ç§’é—´éš”
                wait_time = 3 - elapsed
                print(f"   â³ ç­‰å¾… {wait_time:.1f}s (é¢‘ç‡æ§åˆ¶)...")
                time.sleep(wait_time)
        self.last_query_time = datetime.now()
    
    def search(self, keyword: str, count: int = 20) -> List[Dict]:
        """
        çŸ¥è¯†æ˜Ÿçƒå…³é”®è¯æœç´¢
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            count: è·å–æ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        # é¢‘ç‡æ§åˆ¶
        self._check_interval()
        
        try:
            # URLç¼–ç å…³é”®è¯
            keyword_encoded = urllib.parse.quote(keyword)
            url = f'https://api.zsxq.com/v2/groups/{self.group_id}/topics?count={count}&keyword={keyword_encoded}'
            
            response = requests.get(url, headers=self.headers, timeout=15)
            
            if response.status_code != 200:
                print(f"   âš ï¸ HTTPé”™è¯¯: {response.status_code}")
                return results
            
            data = response.json()
            
            if not data.get('succeeded'):
                code = data.get('code', 0)
                if code == 1059:
                    print(f"   âš ï¸ è§¦å‘é™æµï¼Œç­‰å¾…åé‡è¯•...")
                    time.sleep(30)
                    return self.search(keyword, count)  # é‡è¯•
                print(f"   âš ï¸ APIé”™è¯¯: code={code}")
                return results
            
            topics = data.get('resp_data', {}).get('topics', [])
            
            for topic in topics:
                talk = topic.get('talk', {})
                text = talk.get('text', '')
                title = talk.get('title', '') or text[:50]
                owner = talk.get('owner', {})
                author = owner.get('name', 'æœªçŸ¥')
                
                results.append({
                    'title': title[:100],
                    'content': text[:300],
                    'author': author,
                    'time': topic.get('create_time', '')[:16],
                    'likes': topic.get('likes_count', 0),
                    'source': 'çŸ¥è¯†æ˜Ÿçƒ',
                    'priority': 2
                })
            
            print(f"   âœ… æ‰¾åˆ° {len(results)} æ¡")
            
        except Exception as e:
            print(f"   âš ï¸ æœç´¢å¤±è´¥: {e}")
        
        return results
    
    def search_industry(self, industry: str, sub_keywords: List[str] = None) -> List[Dict]:
        """
        è¡Œä¸šæ·±åº¦æœç´¢ - å¤šå…³é”®è¯ç»„åˆ
        
        Args:
            industry: è¡Œä¸šä¸»å…³é”®è¯
            sub_keywords: å­å…³é”®è¯åˆ—è¡¨
            
        Returns:
            åˆå¹¶å»é‡åçš„ç»“æœ
        """
        all_results = []
        
        # ä¸»å…³é”®è¯æœç´¢
        print(f"   ğŸ” ä¸»å…³é”®è¯: '{industry}'")
        results = self.search(industry, count=20)
        all_results.extend(results)
        
        # å­å…³é”®è¯æœç´¢
        if sub_keywords:
            for sub_kw in sub_keywords[:3]:  # é™åˆ¶å­å…³é”®è¯æ•°é‡
                print(f"   ğŸ” å­å…³é”®è¯: '{industry} {sub_kw}'")
                results = self.search(f"{industry} {sub_kw}", count=10)
                all_results.extend(results)
                time.sleep(3)  # é¢‘ç‡æ§åˆ¶
        
        # å»é‡
        seen = set()
        unique = []
        for r in all_results:
            key = r['title'][:40]
            if key not in seen:
                seen.add(key)
                unique.append(r)
        
        print(f"   ğŸ“Š å»é‡å: {len(unique)} æ¡")
        return unique


class MultiSourceNewsSearcher:
    """å¤šæºæ–°é—»èšåˆæœç´¢å™¨ v2.0"""
    
    def __init__(self):
        self.all_news = []
        self.sources_stats = {}
        self.zsxq_searcher = ZsxqSearcher()
    
    def search_all(self, keyword: str, stock_code: str = "", stock_name: str = "") -> List[Dict]:
        """
        åŒæ—¶æœç´¢å¤šä¸ªæ•°æ®æºï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            
        Returns:
            åˆå¹¶å»é‡åçš„æ–°é—»åˆ—è¡¨
        """
        self.all_news = []
        self.sources_stats = {}
        
        print(f"\nğŸ” å¯åŠ¨å¤šæºæ–°é—»æœç´¢: {keyword}")
        print("="*60)
        
        # P1: Exaå…¨ç½‘æœç´¢
        print("\nğŸ”¥ [P1] Exaå…¨ç½‘è¯­ä¹‰æœç´¢...")
        # é‡è¦ï¼šExaæœç´¢å¿…é¡»æ‹¼æ¥stock_name+keywordï¼Œç¡®ä¿æœç´¢"æ ‡çš„+å…³é”®è¯"
        if stock_name and keyword:
            exa_keyword = f"{stock_name} {keyword}"
        elif stock_name:
            exa_keyword = stock_name
        else:
            exa_keyword = keyword
        exa_news = self._search_exa(exa_keyword, 8)
        self.all_news.extend(exa_news)
        self.sources_stats['Exaå…¨ç½‘'] = len(exa_news)
        print(f"   âœ… è·å– {len(exa_news)} æ¡ (å…³é”®è¯: {exa_keyword})")
        
        # P2: çŸ¥è¯†æ˜Ÿçƒä¼˜åŒ–ç‰ˆæœç´¢
        print("\nğŸ“š [P2] çŸ¥è¯†æ˜Ÿçƒè°ƒç ”çºªè¦...")
        search_terms = [keyword]
        if stock_name:
            search_terms.insert(0, stock_name)  # ä¼˜å…ˆç”¨è‚¡ç¥¨åæœç´¢
        
        zsxq_news = []
        for term in search_terms[:2]:  # æœ€å¤š2ä¸ªæœç´¢è¯
            results = self.zsxq_searcher.search(term, count=15)
            zsxq_news.extend(results)
            if len(search_terms) > 1:
                time.sleep(3)  # é¢‘ç‡æ§åˆ¶
        
        # å»é‡
        seen_titles = set()
        unique_zsxq = []
        for n in zsxq_news:
            title_key = n['title'][:40]
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_zsxq.append(n)
        
        self.all_news.extend(unique_zsxq)
        self.sources_stats['çŸ¥è¯†æ˜Ÿçƒ'] = len(unique_zsxq)
        print(f"   âœ… è·å– {len(unique_zsxq)} æ¡ï¼ˆå»é‡åï¼‰")
        
        # P3: æ–°æµªè´¢ç»
        print("\nğŸ“° [P3] æ–°æµªè´¢ç»...")
        sina_news = self._search_sina(keyword)
        self.all_news.extend(sina_news)
        self.sources_stats['æ–°æµªè´¢ç»'] = len(sina_news)
        print(f"   âœ… è·å– {len(sina_news)} æ¡")
        
        # P4: åå°”è¡—è§é—»
        print("\nğŸ“° [P4] åå°”è¡—è§é—»...")
        ws_news = self._search_wallstreetcn(keyword)
        self.all_news.extend(ws_news)
        self.sources_stats['åå°”è¡—è§é—»'] = len(ws_news)
        print(f"   âœ… è·å– {len(ws_news)} æ¡")
        
        # æœ€ç»ˆå»é‡
        print("\nğŸ”„ åˆå¹¶å»é‡...")
        unique_news = self._deduplicate(self.all_news)
        print(f"   å»é‡å‰: {len(self.all_news)} æ¡ â†’ å»é‡å: {len(unique_news)} æ¡")
        
        print("="*60)
        return unique_news
    
    def search_industry_chain(self, industry: str, upstream: str = "", downstream: str = "") -> List[Dict]:
        """
        äº§ä¸šé“¾ä¸Šä¸‹æ¸¸æœç´¢
        
        Args:
            industry: è¡Œä¸šåç§°
            upstream: ä¸Šæ¸¸å…³é”®è¯
            downstream: ä¸‹æ¸¸å…³é”®è¯
            
        Returns:
            äº§ä¸šé“¾ç›¸å…³æ–°é—»
        """
        print(f"\nğŸ”— äº§ä¸šé“¾æœç´¢: {industry}")
        print("="*60)
        
        all_news = []
        
        # æœç´¢ä¸»è¡Œä¸š
        print(f"\n1ï¸âƒ£ ä¸»è¡Œä¸š: {industry}")
        news = self.zsxq_searcher.search_industry(industry, ['äº§ä¸šé“¾', 'æ™¯æ°”åº¦', 'ä¾›éœ€'])
        all_news.extend(news)
        
        # æœç´¢ä¸Šæ¸¸
        if upstream:
            print(f"\n2ï¸âƒ£ ä¸Šæ¸¸: {upstream}")
            time.sleep(3)
            news = self.zsxq_searcher.search(upstream, count=15)
            all_news.extend(news)
        
        # æœç´¢ä¸‹æ¸¸
        if downstream:
            print(f"\n3ï¸âƒ£ ä¸‹æ¸¸: {downstream}")
            time.sleep(3)
            news = self.zsxq_searcher.search(downstream, count=15)
            all_news.extend(news)
        
        # å»é‡
        seen = set()
        unique = []
        for n in all_news:
            key = n['title'][:40]
            if key not in seen:
                seen.add(key)
                unique.append(n)
        
        print(f"\nâœ… äº§ä¸šé“¾æœç´¢å®Œæˆ: {len(unique)} æ¡")
        return unique
    
    def _search_exa(self, keyword: str, num: int = 8) -> List[Dict]:
        """Exaå…¨ç½‘æœç´¢"""
        news = []
        try:
            result = subprocess.run(
                ['mcporter', 'call', f'exa.web_search_exa({{"query": "{keyword}", "numResults": {num}}})'],
                capture_output=True, text=True, timeout=20
            )
            if result.returncode == 0:
                titles = re.findall(r'Title: (.+)', result.stdout)
                urls = re.findall(r'URL: (.+)', result.stdout)
                for i, title in enumerate(titles[:num]):
                    news.append({
                        'title': title.strip(),
                        'source': 'Exaå…¨ç½‘',
                        'url': urls[i] if i < len(urls) else '',
                        'priority': 1
                    })
        except Exception as e:
            print(f"   âš ï¸ Exaæœç´¢å¤±è´¥: {e}")
        return news
    
    def _search_sina(self, keyword: str) -> List[Dict]:
        """æ–°æµªè´¢ç»æœç´¢"""
        news = []
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            url = f"https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2516&num=10&keyword={keyword}"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                if 'result' in data and 'data' in data['result']:
                    for item in data['result']['data'][:8]:
                        news.append({
                            'title': item.get('title', ''),
                            'source': 'æ–°æµªè´¢ç»',
                            'url': item.get('url', ''),
                            'priority': 3
                        })
        except Exception as e:
            print(f"   âš ï¸ æ–°æµªè´¢ç»æœç´¢å¤±è´¥: {e}")
        return news
    
    def _search_wallstreetcn(self, keyword: str) -> List[Dict]:
        """åå°”è¡—è§é—»æœç´¢"""
        news = []
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            url = "https://api-one.wallstcn.com/apiv1/content/information-flow?accept=article%2Cad&limit=8"
            
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data.get('code') == 20000 and data.get('data'):
                    items = data['data'].get('items', [])
                    for item in items[:5]:
                        resource = item.get('resource', {})
                        title = resource.get('title', '')
                        if keyword in title or any(k in title for k in keyword.split()[:2]):
                            news.append({
                                'title': title,
                                'source': 'åå°”è¡—è§é—»',
                                'url': '',
                                'priority': 4
                            })
        except Exception as e:
            print(f"   âš ï¸ åå°”è¡—è§é—»æœç´¢å¤±è´¥: {e}")
        return news
    
    def _deduplicate(self, news_list: List[Dict]) -> List[Dict]:
        """æ–°é—»å»é‡"""
        seen = set()
        unique = []
        sorted_news = sorted(news_list, key=lambda x: x.get('priority', 5))
        
        for news in sorted_news:
            title = news.get('title', '')
            simple = ''.join(c for c in title if c.isalnum())[:20]
            if simple and simple not in seen:
                seen.add(simple)
                unique.append(news)
        
        return unique
    
    def format_news_section(self, news_list: List[Dict], max_items: int = 15) -> str:
        """æ ¼å¼åŒ–æ–°é—»ç« èŠ‚"""
        if not news_list:
            return "æš‚æ— ç›¸å…³æ–°é—»"
        
        lines = [
            f"ğŸ“° å¤šæºæ–°é—»èšåˆï¼ˆå…±{len(news_list)}æ¡ï¼Œå»é‡åï¼‰",
            "",
            "**æ•°æ®æºç»Ÿè®¡**ï¼š",
        ]
        
        for source, count in self.sources_stats.items():
            if count > 0:
                lines.append(f"- {source}: {count}æ¡")
        
        lines.extend(["", "**çƒ­é—¨æ–°é—»**ï¼š", ""])
        
        for i, news in enumerate(news_list[:max_items], 1):
            source = news.get('source', 'æœªçŸ¥')
            title = news.get('title', '')[:70]
            author = news.get('author', '')
            
            source_mark = {
                'Exaå…¨ç½‘': 'ğŸ”¥',
                'çŸ¥è¯†æ˜Ÿçƒ': 'ğŸ“š',
                'æ–°æµªè´¢ç»': 'ğŸ“°',
                'åå°”è¡—è§é—»': 'ğŸ“°'
            }.get(source, 'â€¢')
            
            author_info = f" [{author}]" if author and source == 'çŸ¥è¯†æ˜Ÿçƒ' else ""
            lines.append(f"{i}. {source_mark} [{source}]{author_info} {title}...")
        
        return "\n".join(lines)


def search_stock_comprehensive(stock_code: str, stock_name: str, industry: str = "") -> Dict:
    """
    ä¸ªè‚¡å…¨é¢æœç´¢ - åŒ…å«æ‰€æœ‰å¿…è¦å…³é”®è¯åˆ†ç±»
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç 
        stock_name: è‚¡ç¥¨åç§°
        industry: æ‰€å±è¡Œä¸šï¼ˆå¯é€‰ï¼‰
        
    Returns:
        åˆ†ç±»æ±‡æ€»çš„æ–°é—»æ•°æ®
    """
    searcher = MultiSourceNewsSearcher()
    all_results = {}
    
    print(f"\n{'='*80}")
    print(f"ğŸ” å¯åŠ¨ä¸ªè‚¡å…¨é¢æœç´¢: {stock_name} ({stock_code})")
    print(f"{'='*80}")
    
    # 1. åŸºç¡€ä¿¡æ¯æœç´¢
    print("\nğŸ“Œ ã€1/6ã€‘åŸºç¡€ä¸šåŠ¡æœç´¢")
    if industry:
        all_results['åŸºç¡€'] = searcher.search_all(industry, stock_code, stock_name)
    else:
        all_results['åŸºç¡€'] = searcher.search_all("ä¸šåŠ¡ äº§å“", stock_code, stock_name)
    
    # 2. é‡å¤§èµ„æœ¬è¿ä½œï¼ˆå¹¶è´­/æ”¶è´­/å®šå¢/é‡ç»„ï¼‰- å¿…é¡»æœ‰ï¼
    print("\nğŸ“Œ ã€2/6ã€‘é‡å¤§èµ„æœ¬è¿ä½œæœç´¢")
    all_results['èµ„æœ¬è¿ä½œ'] = searcher.search_all("å¹¶è´­ æ”¶è´­ å®šå¢ é‡ç»„ å€Ÿå£³", stock_code, stock_name)
    
    # 3. é£é™©è­¦ç¤ºï¼ˆå‡æŒ/è¿è§„/ç›‘ç®¡/é—®è¯¢å‡½ï¼‰- å¿…é¡»æœ‰ï¼
    print("\nğŸ“Œ ã€3/6ã€‘é£é™©è­¦ç¤ºæœç´¢")
    all_results['é£é™©'] = searcher.search_all("å‡æŒ å¢æŒ è¿è§„ å¤„ç½š ç›‘ç®¡ é—®è¯¢å‡½ å…³æ³¨å‡½ è­¦ç¤ºå‡½", stock_code, stock_name)
    
    # 4. ä¸šåŠ¡é©±åŠ¨ï¼ˆè®¢å•/åˆåŒ/äº§èƒ½/æŠ€æœ¯ï¼‰- å¿…é¡»æœ‰ï¼
    print("\nğŸ“Œ ã€4/6ã€‘ä¸šåŠ¡é©±åŠ¨æœç´¢")
    all_results['ä¸šåŠ¡é©±åŠ¨'] = searcher.search_all("è®¢å• åˆåŒ ä¸­æ ‡ äº§èƒ½æ‰©å¼  æŠ€æœ¯çªç ´ ä¸“åˆ© äº§å“è®¤è¯ å¯¼å…¥", stock_code, stock_name)
    
    # 5. ä¸šç»©ç›¸å…³ï¼ˆé¢„å¢/å˜è„¸/ä¸‹ä¿®/å¿«æŠ¥ï¼‰- å¿…é¡»æœ‰ï¼
    print("\nğŸ“Œ ã€5/6ã€‘ä¸šç»©ç›¸å…³æœç´¢")
    all_results['ä¸šç»©'] = searcher.search_all("ä¸šç»©é¢„å¢ ä¸šç»©å¿«æŠ¥ ä¸šç»©ä¸‹ä¿® ä¸šç»©å˜è„¸ æ‰­äº äºæŸ", stock_code, stock_name)
    
    # 6. èµ„æœ¬å¸‚åœºï¼ˆç ”æŠ¥/è¯„çº§/æœºæ„è°ƒç ”/èµ„é‡‘æµå‘ï¼‰
    print("\nğŸ“Œ ã€6/6ã€‘èµ„æœ¬å¸‚åœºæœç´¢")
    all_results['èµ„æœ¬å¸‚åœº'] = searcher.search_all("ç ”æŠ¥ è¯„çº§ ç›®æ ‡ä»· æœºæ„è°ƒç ” é¾™è™æ¦œ å¤§å®—äº¤æ˜“ åŒ—å‘èµ„é‡‘", stock_code, stock_name)
    
    # ç»Ÿè®¡æ±‡æ€»
    print(f"\n{'='*80}")
    print("ğŸ“Š æœç´¢ç»“æœæ±‡æ€»")
    print(f"{'='*80}")
    total = 0
    for category, news_list in all_results.items():
        count = len(news_list)
        total += count
        print(f"  {category}: {count} æ¡")
    print(f"  {'-'*40}")
    print(f"  æ€»è®¡: {total} æ¡")
    print(f"{'='*80}")
    
    return all_results


def search_multi_source_news(keyword: str, stock_code: str = "", stock_name: str = "") -> str:
    """ä¾¿æ·å‡½æ•°ï¼šå¤šæºæ–°é—»æœç´¢"""
    searcher = MultiSourceNewsSearcher()
    news = searcher.search_all(keyword, stock_code, stock_name)
    return searcher.format_news_section(news)


def search_industry_chain_news(industry: str, upstream: str = "", downstream: str = "") -> str:
    """ä¾¿æ·å‡½æ•°ï¼šäº§ä¸šé“¾æœç´¢"""
    searcher = MultiSourceNewsSearcher()
    news = searcher.search_industry_chain(industry, upstream, downstream)
    return searcher.format_news_section(news)


if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•å¤šæºæ–°é—»æœç´¢ v2.0")
    print("="*60)
    
    # æµ‹è¯•ä¸ªè‚¡æœç´¢
    print("\nã€æµ‹è¯•1ã€‘ä¸ªè‚¡æœç´¢ï¼šåæ‡‹ç§‘æŠ€")
    result = search_multi_source_news("åæ‡‹ç§‘æŠ€", "603306.SH", "åæ‡‹ç§‘æŠ€")
    print(result[:1000])
    
    print("\n...")
    print("\nâœ… æµ‹è¯•å®Œæˆ!")
