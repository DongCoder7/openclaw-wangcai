#!/usr/bin/env python3
"""
å¤šæºæ–°é—»èšåˆæœç´¢æ¨¡å—
åŒæ—¶æœç´¢Exaã€çŸ¥è¯†æ˜Ÿçƒã€æ–°æµªè´¢ç»ç­‰å¤šä¸ªæ•°æ®æº
"""

import sys
import subprocess
import re
import requests
from typing import List, Dict, Optional
from datetime import datetime

sys.path.insert(0, '/root/.openclaw/workspace/tools')
sys.path.insert(0, '/root/.openclaw/workspace')


class MultiSourceNewsSearcher:
    """å¤šæºæ–°é—»èšåˆæœç´¢å™¨"""
    
    def __init__(self):
        self.all_news = []
        self.sources_stats = {}
    
    def search_all(self, keyword: str, stock_code: str = "", stock_name: str = "") -> List[Dict]:
        """
        åŒæ—¶æœç´¢å¤šä¸ªæ•°æ®æº
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºçŸ¥è¯†æ˜Ÿçƒæœç´¢ï¼‰
            stock_name: è‚¡ç¥¨åç§°ï¼ˆç”¨äºçŸ¥è¯†æ˜Ÿçƒæœç´¢ï¼‰
            
        Returns:
            åˆå¹¶å»é‡åçš„æ–°é—»åˆ—è¡¨
        """
        self.all_news = []
        self.sources_stats = {}
        
        print(f"\nğŸ” å¯åŠ¨å¤šæºæ–°é—»æœç´¢: {keyword}")
        print("="*60)
        
        # P1: Exaå…¨ç½‘æœç´¢
        print("\nğŸ”¥ [P1] Exaå…¨ç½‘è¯­ä¹‰æœç´¢...")
        exa_news = self._search_exa(keyword)
        self.all_news.extend(exa_news)
        self.sources_stats['Exaå…¨ç½‘'] = len(exa_news)
        print(f"   âœ… è·å– {len(exa_news)} æ¡")
        
        # P2: çŸ¥è¯†æ˜Ÿçƒè°ƒç ”çºªè¦
        if stock_code or stock_name:
            print("\nğŸ“š [P2] çŸ¥è¯†æ˜Ÿçƒè°ƒç ”çºªè¦...")
            zsxq_news = self._search_zsxq(stock_code, stock_name)
            self.all_news.extend(zsxq_news)
            self.sources_stats['çŸ¥è¯†æ˜Ÿçƒ'] = len(zsxq_news)
            print(f"   âœ… è·å– {len(zsxq_news)} æ¡")
        
        # P3: æ–°æµªè´¢ç»
        print("\nğŸ“° [P3] æ–°æµªè´¢ç»...")
        sina_news = self._search_sina(keyword)
        self.all_news.extend(sina_news)
        self.sources_stats['æ–°æµªè´¢ç»'] = len(sina_news)
        print(f"   âœ… è·å– {len(sina_news)} æ¡")
        
        # P4: åå°”è¡—è§é—»
        print("\nğŸ“° [P4] åå°”è¡—è§é—»...")
        ws_news = self._search_wallstreetcn(keyword)
        self.all_news.extend(ws_news)
        self.sources_stats['åå°”è¡—è§é—»'] = len(ws_news)
        print(f"   âœ… è·å– {len(ws_news)} æ¡")
        
        # å»é‡
        print("\nğŸ”„ åˆå¹¶å»é‡...")
        unique_news = self._deduplicate(self.all_news)
        print(f"   å»é‡å‰: {len(self.all_news)} æ¡ â†’ å»é‡å: {len(unique_news)} æ¡")
        
        print("="*60)
        return unique_news
    
    def _search_exa(self, keyword: str, num: int = 8) -> List[Dict]:
        """Exaå…¨ç½‘æœç´¢"""
        news = []
        try:
            result = subprocess.run(
                ['mcporter', 'call', f'exa.web_search_exa({{"query": "{keyword}", "numResults": {num}}})'],
                capture_output=True, text=True, timeout=20
            )
            if result.returncode == 0:
                titles = re.findall(r'Title: (.+)', result.stdout)
                urls = re.findall(r'URL: (.+)', result.stdout)
                for i, title in enumerate(titles[:num]):
                    news.append({
                        'title': title.strip(),
                        'source': 'Exaå…¨ç½‘',
                        'url': urls[i] if i < len(urls) else '',
                        'priority': 1
                    })
        except Exception as e:
            print(f"   âš ï¸ Exaæœç´¢å¤±è´¥: {e}")
        return news
    
    def _search_zsxq(self, stock_code: str, stock_name: str) -> List[Dict]:
        """çŸ¥è¯†æ˜Ÿçƒè°ƒç ”çºªè¦æœç´¢"""
        news = []
        try:
            from zsxq_fetcher import search_industry_info
            
            # ä¼˜å…ˆä½¿ç”¨è‚¡ç¥¨åç§°æœç´¢
            search_term = stock_name if stock_name else stock_code
            topics = search_industry_info(search_term, count=5)
            
            if topics:
                for topic in topics[:5]:
                    news.append({
                        'title': topic.get('title', '')[:100],
                        'source': 'çŸ¥è¯†æ˜Ÿçƒ',
                        'url': topic.get('url', ''),
                        'priority': 2
                    })
        except Exception as e:
            print(f"   âš ï¸ çŸ¥è¯†æ˜Ÿçƒæœç´¢å¤±è´¥: {e}")
        return news
    
    def _search_sina(self, keyword: str) -> List[Dict]:
        """æ–°æµªè´¢ç»æœç´¢"""
        news = []
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            
            # æ–°æµªè´¢ç»API
            url = f"https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2516&num=10&keyword={keyword}"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                if 'result' in data and 'data' in data['result']:
                    for item in data['result']['data'][:8]:
                        news.append({
                            'title': item.get('title', ''),
                            'source': 'æ–°æµªè´¢ç»',
                            'url': item.get('url', ''),
                            'priority': 3
                        })
        except Exception as e:
            print(f"   âš ï¸ æ–°æµªè´¢ç»æœç´¢å¤±è´¥: {e}")
        return news
    
    def _search_wallstreetcn(self, keyword: str) -> List[Dict]:
        """åå°”è¡—è§é—»æœç´¢"""
        news = []
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            url = f"https://api-one.wallstcn.com/apiv1/content/information-flow?accept=article%2Cad&limit=8"
            
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data.get('code') == 20000 and data.get('data'):
                    items = data['data'].get('items', [])
                    for item in items[:5]:
                        resource = item.get('resource', {})
                        title = resource.get('title', '')
                        # ç®€å•è¿‡æ»¤ç›¸å…³æ€§
                        if keyword in title or any(k in title for k in keyword.split()[:2]):
                            news.append({
                                'title': title,
                                'source': 'åå°”è¡—è§é—»',
                                'url': '',
                                'priority': 4
                            })
        except Exception as e:
            print(f"   âš ï¸ åå°”è¡—è§é—»æœç´¢å¤±è´¥: {e}")
        return news
    
    def _deduplicate(self, news_list: List[Dict]) -> List[Dict]:
        """æ–°é—»å»é‡ï¼ˆåŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰"""
        seen = set()
        unique = []
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_news = sorted(news_list, key=lambda x: x.get('priority', 5))
        
        for news in sorted_news:
            title = news.get('title', '')
            # ç®€åŒ–æ ‡é¢˜ç”¨äºå»é‡
            simple = ''.join(c for c in title if c.isalnum())[:20]
            if simple and simple not in seen:
                seen.add(simple)
                unique.append(news)
        
        return unique
    
    def format_news_section(self, news_list: List[Dict], max_items: int = 10) -> str:
        """æ ¼å¼åŒ–æ–°é—»ç« èŠ‚"""
        if not news_list:
            return "æš‚æ— ç›¸å…³æ–°é—»"
        
        lines = [
            f"ğŸ“° å¤šæºæ–°é—»èšåˆï¼ˆå…±{len(news_list)}æ¡ï¼Œå»é‡åï¼‰",
            "",
            "**æ•°æ®æºç»Ÿè®¡**ï¼š",
        ]
        
        for source, count in self.sources_stats.items():
            if count > 0:
                lines.append(f"- {source}: {count}æ¡")
        
        lines.extend([
            "",
            "**çƒ­é—¨æ–°é—»**ï¼š",
            "",
        ])
        
        for i, news in enumerate(news_list[:max_items], 1):
            source = news.get('source', 'æœªçŸ¥')
            title = news.get('title', '')[:70]
            
            # æ¥æºæ ‡è®°
            source_mark = {
                'Exaå…¨ç½‘': 'ğŸ”¥',
                'çŸ¥è¯†æ˜Ÿçƒ': 'ğŸ“š',
                'æ–°æµªè´¢ç»': 'ğŸ“°',
                'åå°”è¡—è§é—»': 'ğŸ“°'
            }.get(source, 'â€¢')
            
            lines.append(f"{i}. {source_mark} [{source}] {title}...")
        
        return "\n".join(lines)


# ä¾¿æ·å‡½æ•°
def search_multi_source_news(keyword: str, stock_code: str = "", stock_name: str = "") -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šå¤šæºæ–°é—»æœç´¢
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        stock_code: è‚¡ç¥¨ä»£ç 
        stock_name: è‚¡ç¥¨åç§°
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ±‡æ€»
    """
    searcher = MultiSourceNewsSearcher()
    news = searcher.search_all(keyword, stock_code, stock_name)
    return searcher.format_news_section(news)


if __name__ == "__main__":
    # æµ‹è¯•
    print("ğŸ§ª æµ‹è¯•å¤šæºæ–°é—»æœç´¢")
    print("="*60)
    
    result = search_multi_source_news("åæ‡‹ç§‘æŠ€", "603306.SH", "åæ‡‹ç§‘æŠ€")
    print(result[:1500])
    print("\n... [åç»­å†…å®¹çœç•¥] ...")
    print("\nâœ… æµ‹è¯•å®Œæˆ!")
