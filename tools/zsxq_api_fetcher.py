#!/usr/bin/env python3
"""
çŸ¥è¯†æ˜ŸçƒAPIçˆ¬è™« - è°ƒç ”çºªè¦é¢‘é“æœ€è¿‘ä¸€å‘¨ä¿¡æ¯æŠ“å–
å‚è€ƒ: https://cloud.tencent.com/developer/article/2627228

åŠŸèƒ½:
1. æŠ“å–æŒ‡å®šæ˜Ÿçƒæœ€è¿‘ä¸€å‘¨çš„ä¸»é¢˜
2. åªæŠ“å–"è°ƒç ”çºªè¦"é¢‘é“çš„å†…å®¹
3. æŒ‰å¤©ç»Ÿè®¡ä¿¡æ¯é‡
4. è¾“å‡ºJSONå’ŒMarkdownæŠ¥å‘Š
"""

import requests
import hashlib
import time
import json
import os
import sys
from urllib.parse import urlencode
from datetime import datetime, timedelta
from collections import defaultdict

# ============ é…ç½®åŒºåŸŸ ============

# æ˜ŸçƒID (ä»URLä¸­è·å–ï¼Œå¦‚ https://wx.zsxq.com/group/28855458518111)
GROUP_ID = "28855458518111"  # è¯·æ›¿æ¢ä¸ºä½ çš„æ˜ŸçƒID

# ç›®æ ‡é¢‘é“åç§° (åªæŠ“å–è¿™ä¸ªé¢‘é“çš„å†…å®¹)
TARGET_CHANNEL = "è°ƒç ”çºªè¦"

# Cookieé…ç½® - å¿…é¡»æ‰‹åŠ¨ä»æµè§ˆå™¨è·å–
# è·å–æ–¹æ³•:
# 1. ç™»å½• https://wx.zsxq.com
# 2. F12æ‰“å¼€å¼€å‘è€…å·¥å…· -> Network
# 3. åˆ·æ–°é¡µé¢ï¼Œæ‰¾åˆ° api.zsxq.com çš„è¯·æ±‚
# 4. å¤åˆ¶ Cookie å­—æ®µä¸­çš„ zsxq_access_token
ZSXQ_COOKIE = os.getenv("ZSXQ_COOKIE", "")

# APIåŸºç¡€é…ç½®
BASE_URL = "https://api.zsxq.com"
APP_VERSION = "3.11.0"
PLATFORM = "ios"
SECRET = "zsxqapi2020"  # çŸ¥è¯†æ˜Ÿçƒå†…ç½®å¯†é’¥

# è¾“å‡ºç›®å½•
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "..", "data", "zsxq")


class ZsxqApiSpider:
    """çŸ¥è¯†æ˜ŸçƒAPIçˆ¬è™«"""
    
    def __init__(self, cookie: str):
        if not cookie:
            raise ValueError("Cookieä¸èƒ½ä¸ºç©ºï¼Œè¯·è®¾ç½® ZSXQ_COOKIE ç¯å¢ƒå˜é‡æˆ–ç›´æ¥ä¿®æ”¹è„šæœ¬")
        
        self.cookie = cookie
        self.base_url = BASE_URL
        self.app_version = APP_VERSION
        self.platform = PLATFORM
        self.secret = SECRET
        
        # åŸºç¡€è¯·æ±‚å¤´
        self.headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9",
            "Connection": "keep-alive",
            "Cookie": self.cookie,
            "Origin": "https://wx.zsxq.com",
            "Referer": "https://wx.zsxq.com/"
        }
    
    def generate_signature(self, path: str, params: dict = None) -> tuple:
        """
        ç”ŸæˆçŸ¥è¯†æ˜ŸçƒAPIç­¾å
        
        ç­¾åè§„åˆ™:
        1. å…¬å…±å‚æ•°: app_version, platform, timestamp(æ¯«ç§’)
        2. åˆå¹¶ä¸šåŠ¡å‚æ•°ï¼ŒæŒ‰é”®åå‡åºæ’åº
        3. æ‹¼æ¥: path&key1=value1&key2=value2&secret
        4. MD5åŠ å¯†ï¼Œ32ä½å°å†™
        """
        # 1. åˆå§‹åŒ–å…¬å…±å‚æ•°
        common_params = {
            "app_version": self.app_version,
            "platform": self.platform,
            "timestamp": str(int(time.time() * 1000))  # æ¯«ç§’çº§æ—¶é—´æˆ³
        }
        
        # 2. åˆå¹¶å¹¶æ’åºå‚æ•°
        all_params = common_params.copy()
        if params and isinstance(params, dict):
            all_params.update(params)
        
        sorted_params = sorted(all_params.items(), key=lambda x: x[0])
        params_str = urlencode(sorted_params)
        
        # 3. æ‹¼æ¥å¾…ç­¾åå­—ç¬¦ä¸²
        sign_str = f"{path}&{params_str}&{self.secret}"
        
        # 4. MD5åŠ å¯†
        md5 = hashlib.md5()
        md5.update(sign_str.encode("utf-8"))
        signature = md5.hexdigest()
        
        return signature, common_params["timestamp"]
    
    def send_get_request(self, path: str, params: dict = None) -> dict:
        """å‘é€GETè¯·æ±‚"""
        signature, timestamp = self.generate_signature(path, params)
        
        # æ›´æ–°è¯·æ±‚å¤´
        self.headers["X-Signature"] = signature
        self.headers["X-Timestamp"] = timestamp
        
        url = f"{self.base_url}{path}"
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=15)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"âŒ GETè¯·æ±‚å¤±è´¥: {path}, é”™è¯¯: {e}")
            return None
    
    def get_group_info(self, group_id: str) -> dict:
        """è·å–æ˜ŸçƒåŸºæœ¬ä¿¡æ¯"""
        path = f"/v1/groups/{group_id}"
        response = self.send_get_request(path)
        
        if response and response.get("succeeded"):
            return response.get("resp_data", {}).get("group", {})
        else:
            error = response.get("resp_err", "æœªçŸ¥é”™è¯¯") if response else "è¯·æ±‚å¤±è´¥"
            print(f"âŒ è·å–æ˜Ÿçƒä¿¡æ¯å¤±è´¥: {error}")
            return {}
    
    def get_group_topics(self, group_id: str, count: int = 20, end_time: str = None, scope: str = None) -> tuple:
        """
        è·å–æ˜Ÿçƒä¸»é¢˜åˆ—è¡¨
        
        Args:
            group_id: æ˜ŸçƒID
            count: æ¯é¡µæ•°é‡
            end_time: åˆ†é¡µæ—¶é—´æˆ³
            scope: é¢‘é“ç­›é€‰ (all/combined/file/questions/essence)
        
        Returns:
            (ä¸»é¢˜åˆ—è¡¨, ä¸‹ä¸€é¡µend_time)
        """
        path = f"/v1/groups/{group_id}/topics"
        params = {"count": count}
        
        if end_time:
            params["end_time"] = end_time
        if scope:
            params["scope"] = scope
        
        response = self.send_get_request(path, params)
        
        if response and response.get("succeeded"):
            data = response.get("resp_data", {})
            topics = data.get("topics", [])
            next_end_time = data.get("end_time")
            return topics, next_end_time
        else:
            error = response.get("resp_err", "æœªçŸ¥é”™è¯¯") if response else "è¯·æ±‚å¤±è´¥"
            print(f"âŒ è·å–ä¸»é¢˜åˆ—è¡¨å¤±è´¥: {error}")
            return [], None
    
    def get_topic_detail(self, topic_id: str) -> dict:
        """è·å–ä¸»é¢˜è¯¦æƒ…"""
        path = f"/v1/topics/{topic_id}"
        response = self.send_get_request(path)
        
        if response and response.get("succeeded"):
            return response.get("resp_data", {}).get("topic", {})
        else:
            return {}
    
    def get_channels(self, group_id: str) -> list:
        """è·å–æ˜Ÿçƒé¢‘é“åˆ—è¡¨"""
        path = f"/v1/groups/{group_id}/tags"
        response = self.send_get_request(path)
        
        if response and response.get("succeeded"):
            return response.get("resp_data", {}).get("tags", [])
        return []


def parse_topic_time(create_time: str) -> datetime:
    """è§£æä¸»é¢˜åˆ›å»ºæ—¶é—´"""
    # æ ¼å¼: 2024-03-01T10:30:00.000+0800
    try:
        # å»æ‰æ—¶åŒºä¿¡æ¯ï¼Œå¤„ç†å¾®ç§’
        dt_str = create_time[:19]  # å–å‰19ä¸ªå­—ç¬¦: 2024-03-01T10:30:00
        return datetime.strptime(dt_str, "%Y-%m-%dT%H:%M:%S")
    except:
        return None


def extract_topic_summary(topic: dict) -> dict:
    """æå–ä¸»é¢˜æ‘˜è¦ä¿¡æ¯"""
    topic_id = topic.get("topic_id", "")
    create_time = topic.get("create_time", "")
    title = ""
    content = ""
    author = ""
    channel = ""
    
    # è§£ææ—¶é—´
    dt = parse_topic_time(create_time)
    date_str = dt.strftime("%Y-%m-%d") if dt else ""
    
    # è·å–ä½œè€…
    author_info = topic.get("owner", {})
    author = author_info.get("name", "")
    
    # è·å–é¢‘é“/æ ‡ç­¾
    tags = topic.get("tags", [])
    if tags:
        channel = tags[0].get("name", "")
    
    # è·å–å†…å®¹
    talk = topic.get("talk", {})
    if talk:
        title = talk.get("title", "")
        content = talk.get("text", "")
    
    # é—®ç­”ç±»å‹
    question = topic.get("question", {})
    if question:
        title = question.get("title", "")
        content = question.get("text", "")
    
    # æ–‡ä»¶ç±»å‹
    file_info = topic.get("files", [{}])[0]
    if file_info and not content:
        title = file_info.get("name", "")
        content = f"[æ–‡ä»¶] {file_info.get('name', '')}"
    
    return {
        "topic_id": topic_id,
        "date": date_str,
        "create_time": create_time,
        "author": author,
        "channel": channel,
        "title": title[:100] + "..." if len(title) > 100 else title,
        "content": content[:200] + "..." if len(content) > 200 else content,
        "type": topic.get("type", "")
    }


def fetch_recent_topics(spider: ZsxqApiSpider, group_id: str, target_channel: str = None, days: int = 7) -> list:
    """
    æŠ“å–æœ€è¿‘Nå¤©çš„ä¸»é¢˜
    
    Args:
        spider: APIçˆ¬è™«å®ä¾‹
        group_id: æ˜ŸçƒID
        target_channel: ç›®æ ‡é¢‘é“åç§° (Noneè¡¨ç¤ºå…¨éƒ¨)
        days: æŠ“å–æœ€è¿‘å¤šå°‘å¤©
    
    Returns:
        ä¸»é¢˜åˆ—è¡¨
    """
    print(f"ğŸš€ å¼€å§‹æŠ“å–æœ€è¿‘ {days} å¤©çš„ä¸»é¢˜...")
    if target_channel:
        print(f"ğŸ¯ åªæŠ“å–é¢‘é“: {target_channel}")
    
    # è®¡ç®—æˆªæ­¢æ—¥æœŸ
    cutoff_date = datetime.now() - timedelta(days=days)
    print(f"ğŸ“… æˆªæ­¢æ—¥æœŸ: {cutoff_date.strftime('%Y-%m-%d')}")
    
    all_topics = []
    end_time = None
    page = 1
    reached_cutoff = False
    
    while True:
        print(f"ğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page} é¡µ...")
        topics, next_end_time = spider.get_group_topics(group_id, count=20, end_time=end_time)
        
        if not topics:
            print("âœ… æ²¡æœ‰æ›´å¤šä¸»é¢˜")
            break
        
        for topic in topics:
            create_time = topic.get("create_time", "")
            dt = parse_topic_time(create_time)
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æˆªæ­¢æ—¥æœŸ
            if dt and dt < cutoff_date:
                print(f"ğŸ“… å·²åˆ°è¾¾æˆªæ­¢æ—¥æœŸ ({dt.strftime('%Y-%m-%d')})")
                reached_cutoff = True
                break
            
            # æå–ä¸»é¢˜ä¿¡æ¯
            summary = extract_topic_summary(topic)
            
            # é¢‘é“ç­›é€‰
            if target_channel:
                if summary["channel"] != target_channel:
                    continue
            
            all_topics.append(summary)
        
        if reached_cutoff:
            break
        
        if not next_end_time:
            break
        
        end_time = next_end_time
        page += 1        
        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    print(f"âœ… å…±æŠ“å– {len(all_topics)} æ¡ä¸»é¢˜")
    return all_topics


def generate_daily_stats(topics: list) -> dict:
    """ç”Ÿæˆæ¯å¤©çš„ä¿¡æ¯é‡ç»Ÿè®¡"""
    stats = defaultdict(lambda: {"count": 0, "topics": []})
    
    for topic in topics:
        date = topic.get("date")
        if date:
            stats[date]["count"] += 1
            stats[date]["topics"].append(topic)
    
    # æŒ‰æ—¥æœŸæ’åº
    return dict(sorted(stats.items(), key=lambda x: x[0], reverse=True))


def save_json(data: dict, filename: str):
    """ä¿å­˜JSONæ–‡ä»¶"""
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ JSONå·²ä¿å­˜: {filename}")


def generate_markdown_report(stats: dict, group_name: str, days: int, output_file: str):
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"# ğŸ“Š çŸ¥è¯†æ˜Ÿçƒä¿¡æ¯ç»Ÿè®¡æŠ¥å‘Š\n\n")
        f.write(f"**æ˜Ÿçƒ**: {group_name}\n\n")
        f.write(f"**ç»Ÿè®¡å‘¨æœŸ**: æœ€è¿‘ {days} å¤©\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"---\n\n")
        
        # æ€»ä½“ç»Ÿè®¡
        total = sum(s["count"] for s in stats.values())
        f.write(f"## ğŸ“ˆ æ€»ä½“ç»Ÿè®¡\n\n")
        f.write(f"- **æ€»ä¿¡æ¯é‡**: {total} æ¡\n")
        f.write(f"- **ç»Ÿè®¡å¤©æ•°**: {len(stats)} å¤©\n")
        f.write(f"- **æ—¥å‡ä¿¡æ¯é‡**: {total / len(stats) if stats else 0:.1f} æ¡\n\n")
        
        # æ¯å¤©ç»Ÿè®¡
        f.write(f"## ğŸ“… æ¯æ—¥ä¿¡æ¯é‡\n\n")
        f.write(f"| æ—¥æœŸ | ä¿¡æ¯é‡ | å æ¯” |\n")
        f.write(f"|------|--------|------|\n")
        
        for date, data in stats.items():
            count = data["count"]
            percentage = (count / total * 100) if total > 0 else 0
            bar = "â–ˆ" * int(percentage / 5)  # æ¯5%ä¸€ä¸ªæ–¹å—
            f.write(f"| {date} | {count} æ¡ | {bar} {percentage:.1f}% |\n")
        
        f.write(f"\n")
        
        # è¯¦ç»†åˆ—è¡¨
        f.write(f"## ğŸ“ è¯¦ç»†å†…å®¹\n\n")
        
        for date, data in stats.items():
            f.write(f"### {date} ({data['count']} æ¡)\n\n")
            
            for i, topic in enumerate(data["topics"][:10], 1):  # æ¯å¤©æœ€å¤šæ˜¾ç¤º10æ¡
                f.write(f"**{i}. {topic['title'] or 'æ— æ ‡é¢˜'}**\n\n")
                f.write(f"- ä½œè€…: {topic['author']}\n")
                f.write(f"- æ—¶é—´: {topic['create_time']}\n")
                if topic['channel']:
                    f.write(f"- é¢‘é“: {topic['channel']}\n")
                f.write(f"- å†…å®¹: {topic['content'][:150]}...\n\n")
            
            if len(data["topics"]) > 10:
                f.write(f"*... è¿˜æœ‰ {len(data['topics']) - 10} æ¡å†…å®¹ ...*\n\n")
        
        f.write(f"---\n\n")
        f.write(f"*æŠ¥å‘Šç”±è‡ªåŠ¨è„šæœ¬ç”Ÿæˆ*\n")
    
    print(f"ğŸ“ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸŒŸ çŸ¥è¯†æ˜Ÿçƒä¿¡æ¯æŠ“å–å·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥Cookie
    cookie = ZSXQ_COOKIE
    if not cookie:
        print("""
âŒ é”™è¯¯: æœªè®¾ç½®Cookie

è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è®¾ç½®Cookie:

æ–¹æ³•1 - ç¯å¢ƒå˜é‡ (æ¨è):
  export ZSXQ_COOKIE="zsxq_access_token=ä½ çš„tokenå€¼"

æ–¹æ³•2 - ä¿®æ”¹è„šæœ¬:
  ç¼–è¾‘æœ¬è„šæœ¬ï¼Œä¿®æ”¹ ZSXQ_COOKIE å˜é‡

è·å–Cookieæ–¹æ³•:
  1. ç™»å½• https://wx.zsxq.com
  2. F12æ‰“å¼€å¼€å‘è€…å·¥å…· -> Network
  3. åˆ·æ–°é¡µé¢ï¼Œæ‰¾åˆ° api.zsxq.com çš„è¯·æ±‚
  4. å¤åˆ¶ Cookie å­—æ®µçš„å®Œæ•´å†…å®¹
        """)
        sys.exit(1)
    
    # åˆå§‹åŒ–çˆ¬è™«
    try:
        spider = ZsxqApiSpider(cookie)
    except ValueError as e:
        print(f"âŒ {e}")
        sys.exit(1)
    
    # è·å–æ˜Ÿçƒä¿¡æ¯
    print(f"\nğŸ” è·å–æ˜Ÿçƒä¿¡æ¯ (ID: {GROUP_ID})...")
    group_info = spider.get_group_info(GROUP_ID)
    group_name = group_info.get("name", "æœªçŸ¥æ˜Ÿçƒ")
    print(f"âœ… æ˜Ÿçƒåç§°: {group_name}")
    
    # æ˜¾ç¤ºé¢‘é“åˆ—è¡¨
    print(f"\nğŸ“‹ é¢‘é“åˆ—è¡¨:")
    channels = spider.get_channels(GROUP_ID)
    if channels:
        for ch in channels:
            print(f"  - {ch.get('name', 'æœªå‘½å')} (ID: {ch.get('id', 'N/A')})")
    else:
        print("  (æœªè·å–åˆ°é¢‘é“åˆ—è¡¨ï¼ŒAPIå¯èƒ½ä¸æ”¯æŒ)")
    
    # æŠ“å–æœ€è¿‘7å¤©æ•°æ®
    print(f"\n" + "=" * 60)
    topics = fetch_recent_topics(spider, GROUP_ID, TARGET_CHANNEL, days=7)
    
    if not topics:
        print("âš ï¸ æœªæŠ“å–åˆ°ä»»ä½•ä¸»é¢˜ï¼Œè¯·æ£€æŸ¥:")
        print("  1. Cookieæ˜¯å¦æœ‰æ•ˆ")
        print("  2. æ˜¯å¦æœ‰æƒé™è®¿é—®è¯¥æ˜Ÿçƒ")
        print("  3. é¢‘é“åç§°æ˜¯å¦æ­£ç¡®")
        sys.exit(1)
    
    # ç”Ÿæˆç»Ÿè®¡
    stats = generate_daily_stats(topics)
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print(f"\n" + "=" * 60)
    print("ğŸ“Š æ¯æ—¥ä¿¡æ¯é‡ç»Ÿè®¡")
    print("=" * 60)
    total = sum(s["count"] for s in stats.values())
    print(f"æ€»è®¡: {total} æ¡\n")
    
    for date, data in stats.items():
        count = data["count"]
        bar = "â–ˆ" * (count * 2)  # ç®€å•çš„å¯è§†åŒ–
        print(f"{date}: {count:3d} æ¡ {bar}")
    
    # å‡†å¤‡è¾“å‡º
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_prefix = f"zsxq_{TARGET_CHANNEL}_{timestamp}" if TARGET_CHANNEL else f"zsxq_all_{timestamp}"
    
    # ä¿å­˜JSON
    json_file = os.path.join(OUTPUT_DIR, f"{output_prefix}.json")
    save_json({
        "meta": {
            "group_id": GROUP_ID,
            "group_name": group_name,
            "target_channel": TARGET_CHANNEL,
            "fetch_time": datetime.now().isoformat(),
            "total_topics": len(topics)
        },
        "stats": stats,
        "topics": topics
    }, json_file)
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_file = os.path.join(OUTPUT_DIR, f"{output_prefix}.md")
    generate_markdown_report(stats, group_name, 7, md_file)
    
    print(f"\nâœ… å®Œæˆ! è¾“å‡ºæ–‡ä»¶:")
    print(f"  - JSON: {json_file}")
    print(f"  - æŠ¥å‘Š: {md_file}")


if __name__ == "__main__":
    main()
