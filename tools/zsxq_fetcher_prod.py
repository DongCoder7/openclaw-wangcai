#!/usr/bin/env python3
"""
çŸ¥è¯†æ˜Ÿçƒç”Ÿäº§çº§çˆ¬è™« - çº¿ç´¢æ± æ•°æ®å±‚ (ç®€åŒ–ç‰ˆ)
æ ¸å¿ƒæœºåˆ¶:
1. end_timeé€†åºå›æº¯ + checkpointæ–­ç‚¹ç»­è·‘
2. seen_idså…¨å±€å»é‡
3. æŒ‰æ—¥æœŸè½ç›˜ raw/YYYY-MM-DD.json
4. é˜²å°ç­–ç•¥: ä½é¢‘éšæœºã€é€€é¿é‡è¯•
5. å…¥åº“å£å¾„: æœ‰æ ‡é¢˜æˆ–æ­£æ–‡å³å…¥åº“

API: /v2 ç‰ˆæœ¬ (æ— éœ€ç­¾å)
"""

import requests
import time
import json
import os
import sys
import random
import logging
import urllib.parse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Tuple, List, Dict

# ============ é…ç½®åŒºåŸŸ ============

GROUP_ID = os.getenv("ZSXQ_GROUP_ID", "28855458518111")

# Cookie (ä»ç°æœ‰è„šæœ¬å¯¼å…¥)
ZSXQ_COOKIE = os.environ.get("ZSXQ_COOKIE") or os.environ.get("ZSXQ_COOKIES")

# æ•°æ®ç›®å½•
DATA_DIR = Path(os.getenv("ZSXQ_DATA_DIR", "/root/.openclaw/workspace/data/zsxq"))
RAW_DIR = DATA_DIR / "raw"
CHECKPOINT_FILE = DATA_DIR / "checkpoint.json"
SEEN_IDS_FILE = DATA_DIR / "seen_ids.txt"

# APIé…ç½®
BASE_URL = "https://api.zsxq.com/v2"

# é˜²å°ç­–ç•¥é…ç½® - ä¿å®ˆç­–ç•¥
REQUEST_MIN_DELAY = 5.0   # æœ€å°è¯·æ±‚é—´éš”(ç§’) - ä¿å®ˆè®¾ç½®
REQUEST_MAX_DELAY = 10.0  # æœ€å¤§è¯·æ±‚é—´éš”(ç§’)
PASS_COOLDOWN = 30        # è½®æ¬¡å†·å´(ç§’)
MAX_RETRIES = 3           # æœ€å¤§é‡è¯•æ¬¡æ•°
BACKOFF_BASE = 5          # é€€é¿åŸºæ•°(ç§’)
CONTINUOUS_ERROR_THRESHOLD = 3  # è¿ç»­å¼‚å¸¸ä¿æŠ¤é€€å‡ºé˜ˆå€¼

# åˆ†é¡µé…ç½®
DEFAULT_PAGE_SIZE = 30  # APIæœ€å¤§æ”¯æŒ30æ¡

# å›è¡¥çª—å£(å¤©)
LOOKBACK_DAYS = int(os.getenv("ZSXQ_LOOKBACK_DAYS", "7"))

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(DATA_DIR / "fetcher.log", encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


class ZsxqFetcher:
    """çŸ¥è¯†æ˜Ÿçƒç”Ÿäº§çº§æŠ“å–å™¨"""
    
    def __init__(self, cookie: str, group_id: str):
        if not cookie:
            raise ValueError("Cookieä¸èƒ½ä¸ºç©º")
        
        self.cookie = cookie
        self.group_id = group_id
        self.base_url = BASE_URL
        
        # ç»Ÿè®¡
        self.stats = {
            "fetched": 0,
            "duplicated": 0,
            "saved": 0,
            "errors": 0,
            "retries": 0
        }
        
        # è¿ç»­å¼‚å¸¸è®¡æ•°
        self.continuous_errors = 0
        
        # åˆå§‹åŒ–ç›®å½•
        self._init_dirs()
        
        # åŠ è½½å·²æŠ“å–ID
        self.seen_ids = self._load_seen_ids()
        
        # è¯·æ±‚å¤´ (ç®€åŒ–ç‰ˆ)
        self.headers = {
            "Cookie": self.cookie,
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "application/json"
        }
    
    def _init_dirs(self):
        """åˆå§‹åŒ–ç›®å½•ç»“æ„"""
        DATA_DIR.mkdir(parents=True, exist_ok=True)
        RAW_DIR.mkdir(exist_ok=True)
        logger.info(f"æ•°æ®ç›®å½•: {DATA_DIR}")
    
    def _load_seen_ids(self) -> set:
        """åŠ è½½å·²æŠ“å–çš„topic_id"""
        if SEEN_IDS_FILE.exists():
            with open(SEEN_IDS_FILE, 'r', encoding='utf-8') as f:
                return set(line.strip() for line in f if line.strip())
        return set()
    
    def _save_seen_id(self, topic_id: str):
        """ä¿å­˜å·²æŠ“å–çš„topic_id"""
        with open(SEEN_IDS_FILE, 'a', encoding='utf-8') as f:
            f.write(f"{topic_id}\n")
        self.seen_ids.add(topic_id)
    
    def _load_checkpoint(self) -> Dict:
        """åŠ è½½æ–­ç‚¹"""
        if CHECKPOINT_FILE.exists():
            with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "last_end_time": None,
            "last_fetch_time": None,
            "fetched_count": 0
        }
    
    def _save_checkpoint(self, end_time: str = None, fetched: int = 0):
        """ä¿å­˜æ–­ç‚¹"""
        checkpoint = {
            "last_end_time": end_time,
            "last_fetch_time": datetime.now().isoformat(),
            "fetched_count": fetched,
            "group_id": self.group_id
        }
        with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)
    
    def _random_delay(self):
        """éšæœºå»¶è¿Ÿï¼Œé˜²å°ç­–ç•¥"""
        delay = random.uniform(REQUEST_MIN_DELAY, REQUEST_MAX_DELAY)
        time.sleep(delay)
    
    def _exponential_backoff(self, attempt: int):
        """æŒ‡æ•°é€€é¿"""
        delay = BACKOFF_BASE * (2 ** attempt) + random.uniform(0, 1)
        logger.info(f"é€€é¿ç­‰å¾…: {delay:.1f}s (å°è¯• {attempt+1}/{MAX_RETRIES})")
        time.sleep(delay)
    
    def send_request(self, path: str, params: dict = None, retry_count: int = 0) -> Optional[dict]:
        """å‘é€è¯·æ±‚ï¼Œå¸¦é€€é¿é‡è¯•"""
        url = f"{self.base_url}{path}"
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            # æ£€æŸ¥ä¸šåŠ¡é”™è¯¯
            if not data.get("succeeded"):
                code = data.get("code", 0)
                logger.warning(f"APIè¿”å›é”™è¯¯: code={code}, msg={data.get('resp_err', 'æœªçŸ¥')}")
                self.stats["errors"] += 1
                self.continuous_errors += 1
                return None
            
            # æˆåŠŸï¼Œé‡ç½®è¿ç»­é”™è¯¯è®¡æ•°
            self.continuous_errors = 0
            return data
            
        except requests.exceptions.RequestException as e:
            logger.warning(f"è¯·æ±‚å¤±è´¥: {e}")
            self.stats["errors"] += 1
            self.continuous_errors += 1
            
            # é€€é¿é‡è¯•
            if retry_count < MAX_RETRIES:
                self._exponential_backoff(retry_count)
                self.stats["retries"] += 1
                return self.send_request(path, params, retry_count + 1)
            
            return None
    
    def get_topics(self, count: int = DEFAULT_PAGE_SIZE, end_time: str = None, retry_count: int = 0) -> Tuple[List[dict], Optional[str]]:
        """è·å–ä¸»é¢˜åˆ—è¡¨ - ä½¿ç”¨URLç¼–ç çš„end_timeåˆ†é¡µï¼Œå¸¦é™æµå¤„ç†"""
        # ä½¿ç”¨ v2 API ç›´æ¥ URL æ„é€ 
        url = f"{self.base_url}/groups/{self.group_id}/topics?count={count}"
        if end_time:
            # end_time éœ€è¦ URL ç¼–ç 
            end_time_encoded = urllib.parse.quote(end_time)
            url += f"&end_time={end_time_encoded}"
        
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            # æ£€æŸ¥ä¸šåŠ¡é”™è¯¯
            if not data.get("succeeded"):
                code = data.get("code", 0)
                error_msg = data.get('resp_err', 'æœªçŸ¥')
                
                # code 1059 æ˜¯é™æµï¼Œéœ€è¦ç­‰å¾…åé‡è¯•
                if code == 1059:
                    if retry_count < MAX_RETRIES:
                        wait_time = 30 * (retry_count + 1)  # 30s, 60s, 90s
                        logger.warning(f"è§¦å‘é™æµ(code=1059)ï¼Œç­‰å¾…{wait_time}såé‡è¯•({retry_count+1}/{MAX_RETRIES})...")
                        time.sleep(wait_time)
                        return self.get_topics(count, end_time, retry_count + 1)
                    else:
                        logger.error(f"é™æµé‡è¯•æ¬¡æ•°è€—å°½ï¼Œåœæ­¢")
                        return [], None
                
                logger.warning(f"APIè¿”å›é”™è¯¯: code={code}, msg={error_msg}")
                self.stats["errors"] += 1
                self.continuous_errors += 1
                return [], None
            
            # æˆåŠŸï¼Œé‡ç½®è¿ç»­é”™è¯¯è®¡æ•°
            self.continuous_errors = 0
            
            resp_data = data.get("resp_data", {})
            topics = resp_data.get("topics", [])
            
            # ä¸‹ä¸€é¡µçš„ end_time æ˜¯æœ€åä¸€æ¡çš„ create_time
            next_end_time = None
            if topics:
                next_end_time = topics[-1].get("create_time")
            
            return topics, next_end_time
            
        except requests.exceptions.RequestException as e:
            logger.warning(f"è¯·æ±‚å¤±è´¥: {e}")
            self.stats["errors"] += 1
            self.continuous_errors += 1
            
            # ç½‘ç»œé”™è¯¯é‡è¯•
            if retry_count < MAX_RETRIES:
                self._exponential_backoff(retry_count)
                return self.get_topics(count, end_time, retry_count + 1)
            
            return [], None
    
    def extract_topic(self, topic: dict) -> Optional[dict]:
        """æå–ä¸»é¢˜ä¿¡æ¯ (å…¥åº“å£å¾„: æœ‰æ ‡é¢˜æˆ–æ­£æ–‡å³å…¥åº“)"""
        topic_id = topic.get("topic_id", "")
        
        # å»é‡æ£€æŸ¥
        if topic_id in self.seen_ids:
            self.stats["duplicated"] += 1
            return None
        
        create_time = topic.get("create_time", "")
        
        # è§£ææ—¥æœŸ
        try:
            dt = datetime.fromisoformat(create_time.replace('Z', '+00:00'))
            date_str = dt.strftime("%Y-%m-%d")
        except:
            date_str = "unknown"
        
        # è·å–ä½œè€…
        talk = topic.get("talk", {})
        owner = talk.get("owner", {}) if talk else {}
        author = owner.get("name", "") if owner else ""
        author_id = owner.get("user_id", "") if owner else ""
        
        # è·å–é¢‘é“/æ ‡ç­¾
        tags = topic.get("tags", [])
        channels = [t.get("name", "") for t in tags]
        
        # æå–æ ‡é¢˜å’Œæ­£æ–‡
        title = ""
        content = ""
        
        if talk:
            title = talk.get("title", "")
            content = talk.get("text", "")
        
        # é—®ç­”
        question = topic.get("question", {})
        if question:
            title = question.get("title", "")
            content = question.get("text", "")
        
        # æ–‡ä»¶
        files = topic.get("files", [])
        if files and not content:
            file_names = [f.get("name", "") for f in files]
            title = file_names[0] if file_names else ""
            content = f"[æ–‡ä»¶] {', '.join(file_names)}"
        
        # å›¾ç‰‡
        images = topic.get("images", [])
        if images and not content:
            content = f"[å›¾ç‰‡] {len(images)}å¼ "
        
        # å…¥åº“å£å¾„: æœ‰æ ‡é¢˜æˆ–æ­£æ–‡å³å…¥åº“
        if not title and not content:
            return None
        
        return {
            "topic_id": topic_id,
            "date": date_str,
            "create_time": create_time,
            "author": author,
            "author_id": author_id,
            "channels": channels,
            "title": title[:200] if title else "",
            "content": content[:500] if content else "",  # é™åˆ¶é•¿åº¦
            "type": topic.get("type", ""),
            "has_attachment": bool(files),
            "image_count": len(images)
        }
    
    def save_to_daily_file(self, topics: List[dict], date: str):
        """æŒ‰æ—¥æœŸè½ç›˜"""
        if not topics:
            return
        
        file_path = RAW_DIR / f"{date}.json"
        
        # è¯»å–å·²æœ‰æ•°æ®
        existing = []
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                existing = json.load(f)
        
        # åˆå¹¶å¹¶å»é‡
        existing_ids = {t["topic_id"] for t in existing}
        new_topics = [t for t in topics if t["topic_id"] not in existing_ids]
        
        if not new_topics:
            return
        
        all_topics = existing + new_topics
        
        # æŒ‰æ—¶é—´æ’åº
        all_topics.sort(key=lambda x: x.get("create_time", ""), reverse=True)
        
        # å†™å…¥
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(all_topics, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ä¿å­˜ {len(new_topics)} æ¡åˆ° {file_path.name} (å…± {len(all_topics)} æ¡)")
        self.stats["saved"] += len(new_topics)
    
    def fetch_with_pagination(self, target_date: str = None, max_pages: int = 100) -> Dict[str, List[dict]]:
        """åˆ†é¡µæŠ“å–ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘"""
        logger.info("=" * 60)
        logger.info(f"ğŸš€ å¼€å§‹æŠ“å–æ˜Ÿçƒ: {self.group_id}")
        if target_date:
            logger.info(f"ğŸ¯ ç›®æ ‡æ—¥æœŸ: {target_date}")
        logger.info("=" * 60)
        
        daily_topics: Dict[str, List[dict]] = {}
        page_count = 0
        stop_reason = "æ­£å¸¸ç»“æŸ"
        
        # ç¬¬ä¸€é¡µä¸ä½¿ç”¨end_time
        end_time = None
        seen_topic_ids = set()  # æœ¬è¿è¡Œå†…å»é‡
        
        while page_count < max_pages:
            # è¿ç»­å¼‚å¸¸ä¿æŠ¤é€€å‡º
            if self.continuous_errors >= CONTINUOUS_ERROR_THRESHOLD:
                stop_reason = f"è¿ç»­å¼‚å¸¸è¾¾åˆ°é˜ˆå€¼({CONTINUOUS_ERROR_THRESHOLD})"
                logger.error(f"âŒ {stop_reason}ï¼Œä¿æŠ¤é€€å‡º")
                break
            
            page_count += 1
            logger.info(f"ğŸ“„ ç¬¬ {page_count} é¡µ (end_time={'æœ‰' if end_time else 'æ— '})")
            
            # è·å–æ•°æ®
            topics, next_end_time = self.get_topics(count=DEFAULT_PAGE_SIZE, end_time=end_time)
            
            if not topics:
                stop_reason = "æ²¡æœ‰æ›´å¤šä¸»é¢˜"
                logger.info(f"âœ… {stop_reason}")
                break
            
            # å¤„ç†æ¯ä¸ªä¸»é¢˜ - æ·»åŠ æœ¬è¿è¡Œå»é‡
            page_new_count = 0
            for topic in topics:
                topic_id = topic.get("topic_id", "")
                
                # è·³è¿‡æœ¬è¿è¡Œå·²å¤„ç†çš„
                if topic_id in seen_topic_ids:
                    continue
                seen_topic_ids.add(topic_id)
                
                extracted = self.extract_topic(topic)
                if extracted:
                    date = extracted["date"]
                    
                    # æ—¥æœŸç­›é€‰
                    if target_date and date != target_date:
                        continue
                    
                    if date not in daily_topics:
                        daily_topics[date] = []
                    
                    daily_topics[date].append(extracted)
                    self._save_seen_id(extracted["topic_id"])
                    page_new_count += 1
                    self.stats["fetched"] += 1
            
            duplicate_count = len(topics) - page_new_count
            logger.info(f"  æœ¬é¡µæ–°æ•°æ®: {page_new_count} æ¡, å»é‡: {duplicate_count} æ¡")
            
            # å¦‚æœæ•´é¡µéƒ½æ˜¯é‡å¤çš„ï¼Œåœæ­¢
            if page_new_count == 0 and len(topics) > 0:
                stop_reason = "æœ¬é¡µå…¨éƒ¨é‡å¤ï¼Œåœæ­¢"
                logger.info(f"â¹ï¸ {stop_reason}")
                break
            
            # æŒ‰æ—¥æœŸè½ç›˜
            for date, topics_list in list(daily_topics.items()):
                if topics_list:
                    self.save_to_daily_file(topics_list, date)
                    daily_topics[date] = []
            
            # ä¿å­˜æ–­ç‚¹
            self._save_checkpoint(next_end_time, self.stats["fetched"])
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if not next_end_time:
                stop_reason = "æ— ä¸‹ä¸€é¡µ"
                logger.info(f"âœ… {stop_reason}")
                break
            
            # æ£€æŸ¥æ—¥æœŸè¾¹ç•Œ
            if target_date:
                earliest_in_page = min(
                    (t.get("create_time", "") for t in topics if t.get("create_time")),
                    default=""
                )
                if earliest_in_page:
                    try:
                        dt = datetime.fromisoformat(earliest_in_page.replace('Z', '+00:00'))
                        target_dt = datetime.strptime(target_date, "%Y-%m-%d")
                        if dt.date() < target_dt.date():
                            stop_reason = "å·²åˆ°è¾¾ç›®æ ‡æ—¥æœŸä¹‹å‰"
                            logger.info(f"âœ… {stop_reason}")
                            break
                    except:
                        pass
            
            # æ›´æ–°æ¸¸æ ‡ - å…³é”®ï¼šä½¿ç”¨next_end_time
            end_time = next_end_time
            
            # éšæœºå»¶è¿Ÿ
            self._random_delay()
        
        if page_count >= max_pages:
            stop_reason = f"è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶({max_pages})"
            logger.info(f"â¹ï¸ {stop_reason}")
        
        # è½®æ¬¡å†·å´
        logger.info(f"â¸ï¸ è½®æ¬¡å†·å´: {PASS_COOLDOWN}s")
        time.sleep(PASS_COOLDOWN)
        
        logger.info(f"ğŸ æŠ“å–ç»“æŸ: {stop_reason}")
        return daily_topics
    
    def generate_daily_report(self) -> str:
        """ç”Ÿæˆæ¯æ—¥ç»Ÿè®¡æŠ¥å‘Š"""
        report_lines = ["ğŸ“Š çŸ¥è¯†æ˜ŸçƒæŠ“å–ç»Ÿè®¡", "=" * 40]
        
        # è¯»å–æ‰€æœ‰æ—¥æœŸæ–‡ä»¶
        daily_counts = {}
        for json_file in sorted(RAW_DIR.glob("*.json")):
            date = json_file.stem
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    daily_counts[date] = len(data)
            except:
                continue
        
        # æŒ‰æ—¥æœŸæ’åº
        for date in sorted(daily_counts.keys(), reverse=True)[:7]:
            count = daily_counts[date]
            bar = "â–ˆ" * min(count // 2, 20)
            report_lines.append(f"{date}: {count:3d} æ¡ {bar}")
        
        total = sum(daily_counts.values())
        report_lines.append("-" * 40)
        report_lines.append(f"æ€»è®¡: {total} æ¡")
        report_lines.append("")
        report_lines.append(f"æœ¬æ¬¡è¿è¡Œ:")
        report_lines.append(f"  æŠ“å–: {self.stats['fetched']}")
        report_lines.append(f"  å»é‡: {self.stats['duplicated']}")
        report_lines.append(f"  ä¿å­˜: {self.stats['saved']}")
        report_lines.append(f"  é”™è¯¯: {self.stats['errors']}")
        report_lines.append(f"  é‡è¯•: {self.stats['retries']}")
        
        return "\n".join(report_lines)


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–
    try:
        fetcher = ZsxqFetcher(ZSXQ_COOKIE, GROUP_ID)
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)
    
    # æŠ“å–
    try:
        fetcher.fetch_with_pagination()
    except KeyboardInterrupt:
        logger.info("â›” ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"æŠ“å–å¼‚å¸¸: {e}")
    
    # è¾“å‡ºæŠ¥å‘Š
    report = fetcher.generate_daily_report()
    print("\n" + report)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = DATA_DIR / f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


if __name__ == "__main__":
    main()
